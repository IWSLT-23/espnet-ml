# rnnlm related
layer: 2
unit: 650
opt: adam        # or adam
batchsize: 64   # batch size in LM training
epoch: 100      # if the data size is large, we can reduce this
patience: 3
maxlen: 100     # if sentence length > lm_maxlen, lm_batchsize is automatically reduced
